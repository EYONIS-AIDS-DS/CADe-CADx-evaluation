{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CADe/CADx Performance Evaluation Tutorial\n",
    "\n",
    "This tutorial demonstrates how to use the repository to evaluate the performance of CADe (Computer-Aided Detection) and CADx (Computer-Aided Diagnosis) systems. \n",
    "It only adresses single \"target\" detection system, nambely binary detection and classification systems (e.g malignant nodule), and computes ROCs, Precison-Recall, FROCs (with severall Operating Points spacifictions), bootstrapps, Confidence Intervals,\n",
    "statistical comparison tests, across sets of detection predictions (...). Simply, it provides a standard and generic CADe/CADx evaluation process, easily usable to any standard case, and easily adaptable to some peculiar requirements. \n",
    "It deserves two purposes that drives 2 independent usage: first to reproduces all the results of the publication (give arXiv link), second to provide evaluation process system for other CADe/CADx.\n",
    "\n",
    "We will walk through the following steps:\n",
    "1. Installing the package and setting up the environment.\n",
    "2. Structure of the Repository\n",
    "3. Reproduce all the paper' figures, performances and tests in a single command from data inputs\n",
    "3. Running the evaluation modules (isolated functions):\n",
    "   - Series-level evaluation.\n",
    "   - Lesion-level evaluation.\n",
    "   - Statistical tests.\n",
    "\n",
    "\n",
    "Let's get started!\n",
    "\n",
    "## 1. INSTALL\n",
    "\n",
    "### 1.1 Install UV - Ultra-fast Python package manager\n",
    "\n",
    "This repository uses UV to manage the dependencies, environment, python version (...) of the repository, and you need to install it first for a direct, fast and easy usage.\n",
    "UV is an ultra-fast Python package manager (see complete on [official UV installation page](https://uv.pypa.io/en/stable/installation/)).\n",
    "To install UV, run the following command in your terminal depending on wheither your system is Linux-based or Windows:\n",
    "\n",
    "Linux / macOS:\n",
    "```bash\n",
    "curl -LsSf https://astral.sh/uv/install.sh | env UV_INSTALL_DIR=\"/usr/local/bin\" sudo -E sh\n",
    "```\n",
    "\n",
    "Windows (PowerShell):\n",
    "```powershell\n",
    "irm (Invoke-WebRequest -Uri https://astral.sh/uv/install.ps1).Content | iex\n",
    "```\n",
    "\n",
    "### 1.2 Install dependencies\n",
    "\n",
    "To install the required dependencies, run the following command in your terminal:\n",
    "\n",
    "```bash\n",
    "uv sync\n",
    "```\n",
    "\n",
    "Activate the virtual environment:\n",
    "\n",
    "```bash\n",
    "uv activate\n",
    "```\n",
    "\n",
    "## 2. Structure of the repository\n",
    "\n",
    "The stucture of the reposity is the following (including the repositories of the output processing that are not commited): \n",
    "\n",
    "```\n",
    "CADe_CADx_evaluation\n",
    "│   README.md\n",
    "│   config_paper.py  \n",
    "│   run_paper_evaluation.py   \n",
    "│\n",
    "└───Data\n",
    "│   │CADe_CADx_evaluate.log  (OUTPUT)\n",
    "│   │\n",
    "│   └───data_series          (INPUT)\n",
    "│   │   │series.csv          \n",
    "│   │  \n",
    "│   └───data_lesions         (INPUT)\n",
    "│   │   │lesions.csv         \n",
    "│   │   │... \n",
    "│   │     \n",
    "│   └───evaluate_series      (OUTPUT)\n",
    "│   │   └─── figure_1  \n",
    "│   │   │     └───4_radiologist_prediction_test3     \n",
    "│   │   │         │roc_curve_with_op_test3.png  \n",
    "│   │   │         │... \n",
    "│   │   │    \n",
    "│   │   └─── ...        \n",
    "│   │     \n",
    "│   └───evaluate_lesions     (OUTPUT)\n",
    "│   │   └─── figure_2  \n",
    "│   │   │     └───4_radiologist_prediction_test3     \n",
    "│   │   │         │froc_curve_with_2_op_test3.png  \n",
    "│   │   │         │... \n",
    "│   │   │    \n",
    "│   │   └─── ...        \n",
    "│   │     \n",
    "│   └───statistical_tests     (OUTPUT)\n",
    "│       └─── figure_1  \n",
    "│       │    │test_results_AUC_model_vs_4radiolog_test3.csv  \n",
    "│       │    │... \n",
    "│       │    \n",
    "│       └─── ...                \n",
    "│   \n",
    "└───evaluate_common            (CODE)\n",
    "│   │roc.py   \n",
    "│   │precision_recall.py\n",
    "│   │sample_size_analysis.py\n",
    "│   │roc_confidence_interval.py\n",
    "│   │plot_score_distribution_benign_cancer.py\n",
    "│   │logger.py\n",
    "│   │sens_spec.py\n",
    "│  \n",
    "└───evaluate_lesions           (CODE)\n",
    "│   │evaluate_lesions.py   \n",
    "│   │froc.py\n",
    "│   │plot_diameter_prediction_distributionss.py\n",
    "│  \n",
    "└───evaluate_series            (CODE)\n",
    "│   │evaluate_series.py   \n",
    "│  \n",
    "└───statistical_tests          (CODE)\n",
    "│   │statistical_tests.py   \n",
    "\n",
    "```\n",
    "\n",
    "### 2.1 Configuration\n",
    "All the parameters of the package are defined in \"config_paper.py\". It includes input and output data paths, name of the predictions, names of the subsets sample of an evaluation, names of the label GT, percentage of the confidence intervals, number of bootstrap, a fast computation option, operating points thresholds and labels. It also defines lists of evalutations specifying predictions, labels, subsets (...) to be runed grouped by figures. To apply the package to new models and dataset, you may either rewrite this configuration and define your own lists of evaluations (complex case of multiple evaluations), or directly run the submodules functions (detailed bellow). By default, this configuration script reproduces all the figures, results of the paper arXiv (to complete).\n",
    "\n",
    "### 2.2 Input\n",
    "The input are stored in .\\data\\data_series and .\\data\\data_lesions directory, for series/patient level and nodules/lesions level input respectively.\n",
    "They are csv files (e.g. series.csv and lesions.csv). Each row is 1 sample, each column is a feature of the sample. There are 4 kind of features used by the evaluation:\n",
    "* predictions: they are numeric (float or int) commonly a probability prediction of a model for the sample, but can be also a (measure) psychophysical assement of a human (e.g. an expert radiologist), or a numerical variable associated with the sample (eg. size) that may have a predictive value of the binary detection-classification.\n",
    "* labels: this are the ground truth associated with the sample. They are binary: either (0,1) or Boolean (true or 1 indicate the postive class, False or 0 indicate the negative class)\n",
    "* identifiers: commonly in medical imaging, patient_id, series_uid, Time_point\n",
    "* features-variables: they are varaibles associated to the sample, on which you may stratify your evalutation (e.g. age, gender, slice-thickness, manufacturer, spiculation...)\n",
    "* test set name: these are boolean variable (indicator functions) that indicate wheither the sample pertain or not to a subset (this has the same role as stratification)\n",
    "In addition for the lesion/nodule level only, there is a required column \"detection_status\" that can take either \"TP\",\"FP\",\"FN\" (for \"True Positive\", \"False Positive\", and \"False Negative\"...) values as a result the output of the pairing of the detection with the GT (see bellow). Note that a \"False negative\" detections are attributed a prediction 0 (or least score) by the classification evaluation algorithm.  \n",
    " \n",
    "### 2.3 modules \"evaluate_lesions\" and \"evaluate_series\"\n",
    "The submodules \"evaluate_lesions\" and \"evaluate_series\" make essentially the same tasks and computation but either at series/patient level or at nodule/lesion level.\n",
    "Those 2 levels are dissociated because patient/series level performance evaluation is straighforward from the GT input (e.g. cancer or not), whereas nodule/lesion level performance requires a prior pairing of the detections with the lesions in the GT.\n",
    "This pairing is not furnished (yet) with the repository, we used standard (for 3D pairing) IoU based pairing with 0.1 threshold as furnished and recommended by [Jaeger et al.](https://arxiv.org/abs/1811.08661).\n",
    "Moreover, lesion/nodule level is also commonly evaluated using FROCs which does not make sens at scan/patient level. Both modules call functions like roc.py (etc..) in the \"evaluate_common\" library.\n",
    "They compute ROCs, FROCs (at lesion levels), etc. (see bellow in the output descriptions).\n",
    "\n",
    "### 2.4 module \"statistical_tests\"\n",
    "The \"statistical_tests\" module is based on Bootstrap methods conceived by [Efron and Tibshirani](https://www.hms.harvard.edu/bss/neuro/bornlab/nb204/statistics/bootstrap.pdf). The bootstrap samples are computed with replacement.  It is recommended because Bootstrap methods are non-parametric (and does not make assumption on the distribution that should be verified, or use parameter that should be fitted), and can be applied generically using a common framework to wide range of observable (on AUC ROC and sensitivity of a given Operating Point of the ROC similarly). It takes a list of paths of pairs of npy saved vectors (in evaluate lesions or series) of n bootstrap samples metric values (AUC, accuracy, sensitivity, specificity....) and run all the list of tests. It implements each time 2 statistical tests using [label](https://docs.scipy.org/doc/scipy/reference/stats.html): \n",
    "* a superiority t-test with unequal variance (one-sided Welch t-test, 1 st prediction vs. 2nd)\n",
    "* a superiority test assuming equal variance (t-test, 1 st prediction vs. 2nd) \n",
    "It  return a csv with the result of all pairs of tests (p value, acceptance (strong, very strong, moderate, rejected)), but also a figure of the 2 bootstrapps distributions. \n",
    "This module can only be runned after \"evaluate_lesions\" and-or \"evaluate_series\" as it depends on their Bootstraps vector .npy file outputs (they are its only inputs). \n",
    "\n",
    "### 2.5 Outputs\n",
    "The input are stored in .\\data\\evaluate_series and .\\data\\evaluate_lesions directory, for series/patient level and nodules/lesions level input respectively.\n",
    "They are csv files, figures both saved in .png and .svg (allowing to further edit them in vectorial format), numpy arrays of bootstrapps samples (for statistical test). \n",
    "The saved figures are:\n",
    "* ROCs, with Operating Points (by default the Youden Index Maximum OP is given, but a list of OP threshold and labels can be given) \n",
    "* Precision Recall curve \n",
    "* the distribution of predictions for each labels\n",
    "* FROCs  (only at lesion level): with operating points at the point closest to 0.5FP/scan and 1FP/scan \n",
    "\n",
    "The saved csv are:\n",
    "* sample sizes: with total sample size, imbalance, and sample size in each class.\n",
    "* \"roc_CI_bootstrap_hanley\" csv: with AUC ROC confidence intervals (lower and upper) obtained using bootstrap methods and using [Hanley & McNeil method](https://pubs.rsna.org/doi/10.1148/radiology.143.1.7063747?url_ver=Z39.88-2003&rfr_id=ori:rid:crossref.org&rfr_dat=cr_pub%20%200pubmed). \n",
    "* \"operating_point_performances\" csv: with, for all operating points, the sens, spec, accuracy, their mean, lower and upper CI over n bootstrapps samples.  \n",
    "* \"operating_point_FROC_scores_at_0.5_and_1_FP_per_scan\"  (only at lesion level): with, for operating points at the point closest to 0.5FP/scan and 1FP/scan , the sens and exact FP/scan, their mean, lower and upper CI over n bootstrapps samples.\n",
    "* \"test_results\" csv (in statistical test folders):  with the result of all pairs of tests (p value, acceptance (strong, very strong, moderate, rejected)), but also a figure of the 2 bootstrapps distributions.\n",
    "\n",
    "The saved numpy (.npy) files are the n_boostrapped vectors of AUC, sens, spec and accuracy.\n",
    "\n",
    "\n",
    "## 3. Reproduce all papers figures, performances and tests in a single command from data inputs\n",
    "\n",
    "The `run_paper_evaluation.py` script is designed to produce all the evaluation performance analyses presented in the paper (give arXiv link). This script integrates all the evaluation modules of package and generates the required outputs of each figures of the paper in the associated directories.\n",
    "At series-patient and at nodule level, it generates all the output listed above for all figures of the paper, and all the statistical tests of the paper.\n",
    "All its parameters are strored in `config_paper.py`, notably the path where to get the input csv data for patients and nodules predictions and the path where to store the output, along with the list of figures, stats and tests to produce. This config is specific to the paper, just rewrite another config keeping the same structure for another set of evaluations.\n",
    "\n",
    "To make the whole paper evaluation, just run the following lines: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  run_paper_evaluation import paper_evaluation\n",
    "fast_computation = True  # Set to True for faster computation during testing/debugging (nb bootstrap samples reduced to 50 and fast FROC computation)\n",
    "paper_evaluation(fast_computation = fast_computation) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the input data are \n",
    "\n",
    "```\n",
    "project\n",
    "│   README.md\n",
    "│   file001.txt    \n",
    "│\n",
    "└───folder1\n",
    "│   │   file011.txt\n",
    "│   │   file012.txt\n",
    "│   │\n",
    "│   └───subfolder1\n",
    "│       │   file111.txt\n",
    "│       │   file112.txt\n",
    "│       │   ...\n",
    "│   \n",
    "└───folder2\n",
    "    │   file021.txt\n",
    "    │   file022.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate_series.evaluate_series import evaluate_serie_main\n",
    "from evaluate_lesions.evaluate_lesions import evaluate_lesions_main\n",
    "from statistical_tests.statistical_tests import statistical_test_main\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Preparing Input Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Input Data\n",
    "# Path to the series data (update this path if necessary)\n",
    "path_to_series_csv = config.path_data_series\n",
    "\n",
    "# Load the series data\n",
    "series_data = pd.read_csv(path_to_series_csv)\n",
    "print(\"Series Data:\")\n",
    "print(series_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect Lesion-Level Data\n",
    "# Path to the lesion data (update this path if necessary)\n",
    "path_to_lesions_csv = config.path_data_lesions\n",
    "\n",
    "# Load the lesion data\n",
    "lesion_data = pd.read_csv(path_to_lesions_csv)\n",
    "print(\"Lesion Data:\")\n",
    "print(lesion_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Running the Evaluation Modules\n",
    "\n",
    "### 3.1 Series-Level Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Series-Level Evaluation\n",
    "# Define parameters for series evaluation\n",
    "path_to_load_csv_serie = config.path_data_series\n",
    "expdir = config.path_model_evaluate_series\n",
    "set_name = \"test1\"  # Update this to the desired test set\n",
    "prediction = \"model_prediction\"\n",
    "label_name = \"label\"\n",
    "operating_point_thresholds = [0]\n",
    "operating_point_labels = [\"Youden Index Max\"]\n",
    "nb_bootstrap_samples = config.nb_bootstrap_samples\n",
    "confidence_threshold = config.confidence_threshold\n",
    "\n",
    "# Run the series evaluation\n",
    "evaluate_serie_main(\n",
    "    path_to_load_csv_serie,\n",
    "    expdir,\n",
    "    set_name,\n",
    "    prediction,\n",
    "    label_name,\n",
    "    operating_point_thresholds,\n",
    "    operating_point_labels,\n",
    "    nb_bootstrap_samples,\n",
    "    confidence_threshold,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Lesion-Level Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Lesion-Level Evaluation\n",
    "# Define parameters for lesion evaluation\n",
    "path_to_load_csv_lesion = config.path_data_lesions\n",
    "expdir = config.path_model_evaluate_lesions\n",
    "set_name = \"test1\"  # Update this to the desired test set\n",
    "prediction = \"model_prediction\"\n",
    "label_name = \"label\"\n",
    "operating_point_thresholds = [0]\n",
    "operating_point_labels = [\"Youden Index Max\"]\n",
    "nb_bootstrap_samples = config.nb_bootstrap_samples\n",
    "confidence_threshold = config.confidence_threshold\n",
    "\n",
    "# Run the lesion evaluation\n",
    "evaluate_lesions_main(\n",
    "    path_to_load_csv_lesion,\n",
    "    expdir,\n",
    "    set_name,\n",
    "    prediction,\n",
    "    label_name,\n",
    "    operating_point_thresholds,\n",
    "    operating_point_labels,\n",
    "    nb_bootstrap_samples,\n",
    "    confidence_threshold,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Statistical Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Statistical Tests\n",
    "# Define parameters for statistical tests\n",
    "list_of_tuples_of_pairs_of_bootstrap_paths = config.list_statistical_tests_figure[0][0]\n",
    "expdir_analysis = config.path_model_statistical_tests\n",
    "\n",
    "# Run the statistical tests\n",
    "statistical_test_main(list_of_tuples_of_pairs_of_bootstrap_paths, expdir_analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Analyzing the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze Results\n",
    "# Path to the output directory\n",
    "output_dir = config.path_model_eval\n",
    "print(f\"Results saved in: {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Example Results\n",
    "# Example: Load and display a Precision-Recall curve\n",
    "example_pr_curve_path = Path(expdir) / \"Precision_Recall_curve_model_test1.png\"\n",
    "if example_pr_curve_path.exists():\n",
    "    img = plt.imread(example_pr_curve_path)\n",
    "    plt.imshow(img)\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(\"Precision-Recall Curve\")\n",
    "    plt.show()\n",
    "else:\n",
    "    print(f\"Precision-Recall curve not found at {example_pr_curve_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this tutorial, we demonstrated how to:\n",
    "1. Set up the environment.\n",
    "2. Prepare input data.\n",
    "3. Run series-level and lesion-level evaluations.\n",
    "4. Perform statistical tests.\n",
    "5. Analyze the results.\n",
    "\n",
    "You can customize the parameters in the `config_nlst.py` file to evaluate different datasets and models.\n",
    "\n",
    "Happy evaluating!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

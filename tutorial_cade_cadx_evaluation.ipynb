{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CADe/CADx Performance Evaluation Tutorial\n",
    "\n",
    "This tutorial demonstrates how to use the repository to evaluate the performance of CADe (Computer-Aided Detection) and CADx (Computer-Aided Diagnosis) systems. \n",
    "It only adresses single \"target\" detection system, nambely binary detection and classification systems (e.g malignant nodule), and computes ROCs, Precison-Recall, FROCs (with severall Operating Points spacifictions), bootstrapps, Confidence Intervals,\n",
    "statistical comparison tests, across sets of detection predictions (...). Simply, it provides a standard and generic CADe/CADx evaluation process, easily usable to any standard case, and easily adaptable to some peculiar requirements. \n",
    "It deserves two purposes that drives 2 independent usage: first to reproduces all the results of the publication (give arXiv link), second to provide evaluation process system for other CADe/CADx.\n",
    "\n",
    "We will walk through the following steps:\n",
    "1. Installing the package and setting up the environment.\n",
    "2. Structure of the Repository\n",
    "3. Reproduce all the paper' figures, performances and tests in a single command from data inputs\n",
    "3. Running the evaluation modules (isolated functions):\n",
    "   - Series-level evaluation.\n",
    "   - Lesion-level evaluation.\n",
    "   - Statistical tests.\n",
    "\n",
    "\n",
    "Let's get started!\n",
    "\n",
    "## 1. INSTALL\n",
    "\n",
    "### 1.1 UV - Ultra-fast Python package manager & Git\n",
    "\n",
    "This repository uses UV to manage the dependencies, environment, python version (...) of the repository, and wenever you would not have it yet, you need to install it first for a direct, fast and easy usage.\n",
    "UV is an ultra-fast Python package manager (see complete on [complete UV installation guidelines](https://uv.pypa.io/en/stable/installation/)).\n",
    "To install UV, run the following command in your terminal depending on wheither your system is Linux-based or Windows:\n",
    "\n",
    "Linux / macOS:\n",
    "```bash\n",
    "curl -LsSf https://astral.sh/uv/install.sh | sh\n",
    "```\n",
    "or if you don't have curl:\n",
    "```bash\n",
    "wget -qO- https://astral.sh/uv/install.sh | sh\n",
    "```\n",
    "\n",
    "Windows (PowerShell):\n",
    "```powershell\n",
    "irm (Invoke-WebRequest -Uri https://astral.sh/uv/install.ps1).Content | iex\n",
    "```\n",
    "\n",
    "Whenever you would not have it yet, install Git ([complete GIT installation guidelines](https://git-scm.com/book/fr/v2/D%C3%A9marrage-rapide-Installation-de-Git)): \n",
    "Linux / macOS:\n",
    "```bash\n",
    "sudo apt-get install git\n",
    "```\n",
    "For windows, download the [installer](https://git-scm.com/download/win) or use the package [Chocolatey Git](https://chocolatey.org/packages/git) for automatic set up.\n",
    "\n",
    "### Clone the repositopry \n",
    "\n",
    "Create the directory where to save the repository on your hard disc and in your terminal go at this repository (called here \"CASe-CADx-evaluation\"), and run the command:\n",
    "```bash\n",
    "git clone https://github.com/EYONIS-AIDS-DS/CADe-CADx-evaluation.git\n",
    "```\n",
    "or use some SSH key, or dowload the zip from \"code\" in Github and unzip it in your directory.\n",
    "\n",
    "### Install dependencies\n",
    "\n",
    "To create the virtual environment:\n",
    "```bash\n",
    "uv venv\n",
    "```\n",
    "To install the required dependencies, run the following command in your terminal at:\n",
    "```bash\n",
    "uv sync\n",
    "```\n",
    "To activate the virtual environment:\n",
    "\n",
    "```bash\n",
    "source .venv/bin/activate\n",
    "```\n",
    "\n",
    "## 2. Structure of the repository\n",
    "\n",
    "The stucture of the reposity is the following (including the repositories of the output processing that are not commited): \n",
    "\n",
    "```\n",
    "CADe_CADx_evaluation\n",
    "│   README.md\n",
    "│   config_paper.py  \n",
    "│   run_paper_evaluation.py   \n",
    "│\n",
    "└───Data\n",
    "│   │CADe_CADx_evaluate.log  (OUTPUT)\n",
    "│   │\n",
    "│   └───data_series          (INPUT)\n",
    "│   │   │series.csv          \n",
    "│   │  \n",
    "│   └───data_lesions         (INPUT)\n",
    "│   │   │lesions.csv         \n",
    "│   │   │... \n",
    "│   │     \n",
    "│   └───evaluate_series      (OUTPUT)\n",
    "│   │   └─── figure_1  \n",
    "│   │   │     └───4_radiologist_prediction_test3     \n",
    "│   │   │         │roc_curve_with_op_test3.png  \n",
    "│   │   │         │... \n",
    "│   │   │    \n",
    "│   │   └─── ...        \n",
    "│   │     \n",
    "│   └───evaluate_lesions     (OUTPUT)\n",
    "│   │   └─── figure_2  \n",
    "│   │   │     └───4_radiologist_prediction_test3     \n",
    "│   │   │         │froc_curve_with_2_op_test3.png  \n",
    "│   │   │         │... \n",
    "│   │   │    \n",
    "│   │   └─── ...        \n",
    "│   │     \n",
    "│   └───statistical_tests     (OUTPUT)\n",
    "│       └─── figure_1  \n",
    "│       │    │test_results_AUC_model_vs_4radiolog_test3.csv  \n",
    "│       │    │... \n",
    "│       │    \n",
    "│       └─── ...                \n",
    "│   \n",
    "└───evaluate_common            (CODE)\n",
    "│   │roc.py   \n",
    "│   │precision_recall.py\n",
    "│   │sample_size_analysis.py\n",
    "│   │roc_confidence_interval.py\n",
    "│   │plot_score_distribution_benign_cancer.py\n",
    "│   │logger.py\n",
    "│   │sens_spec.py\n",
    "│  \n",
    "└───evaluate_lesions           (CODE)\n",
    "│   │evaluate_lesions.py   \n",
    "│   │froc.py\n",
    "│   │plot_diameter_prediction_distributionss.py\n",
    "│  \n",
    "└───evaluate_series            (CODE)\n",
    "│   │evaluate_series.py   \n",
    "│  \n",
    "└───statistical_tests          (CODE)\n",
    "│   │statistical_tests.py   \n",
    "\n",
    "```\n",
    "\n",
    "### 2.1 Configuration\n",
    "All the parameters of the package are defined in \"config_paper.py\". It includes input and output data paths, name of the predictions, names of the subsets sample of an evaluation, names of the label GT, percentage of the confidence intervals, number of bootstrap, a fast computation option, operating points thresholds and labels. It also defines lists of evalutations specifying predictions, labels, subsets (...) to be runed grouped by figures. To apply the package to new models and dataset, you may either rewrite this configuration and define your own lists of evaluations (complex case of multiple evaluations), or directly run the submodules functions (detailed bellow). By default, this configuration script reproduces all the figures, results of the paper arXiv (to complete).\n",
    "\n",
    "### 2.2 Input\n",
    "The input are stored in .\\data\\data_series and .\\data\\data_lesions directory, for series/patient level and nodules/lesions level input respectively.\n",
    "They are csv files (e.g. series.csv and lesions.csv). Each row is 1 sample, each column is a feature of the sample. There are 4 kind of features used by the evaluation:\n",
    "* predictions: they are numeric (float or int) commonly a probability prediction of a model for the sample, but can be also a (measure) psychophysical assement of a human (e.g. an expert radiologist), or a numerical variable associated with the sample (eg. size) that may have a predictive value of the binary detection-classification.\n",
    "* labels: this are the ground truth associated with the sample. They are binary: either (0,1) or Boolean (true or 1 indicate the postive class, False or 0 indicate the negative class)\n",
    "* identifiers: commonly in medical imaging, patient_id, series_uid, Time_point\n",
    "* features-variables: they are varaibles associated to the sample, on which you may stratify your evalutation (e.g. age, gender, slice-thickness, manufacturer, spiculation...)\n",
    "* test set name: these are boolean variable (indicator functions) that indicate wheither the sample pertain or not to a subset (this has the same role as stratification)\n",
    "In addition for the lesion/nodule level only, there is a required column \"detection_status\" that can take either \"TP\",\"FP\",\"FN\" (for \"True Positive\", \"False Positive\", and \"False Negative\"...) values as a result the output of the pairing of the detection with the GT (see bellow). Note that a \"False negative\" detections are attributed a prediction 0 (or least score) by the classification evaluation algorithm.  \n",
    " \n",
    "### 2.3 modules \"evaluate_lesions\" and \"evaluate_series\"\n",
    "The submodules \"evaluate_lesions\" and \"evaluate_series\" make essentially the same tasks and computation but either at series/patient level or at nodule/lesion level.\n",
    "Those 2 levels are dissociated because patient/series level performance evaluation is straighforward from the GT input (e.g. cancer or not), whereas nodule/lesion level performance requires a prior pairing of the detections with the lesions in the GT.\n",
    "This pairing is not furnished (yet) with the repository, we used standard (for 3D pairing) IoU based pairing with 0.1 threshold as furnished and recommended by [Jaeger et al.](https://arxiv.org/abs/1811.08661).\n",
    "Moreover, lesion/nodule level is also commonly evaluated using FROCs which does not make sens at scan/patient level. Both modules call functions like roc.py (etc..) in the \"evaluate_common\" library.\n",
    "They compute ROCs, FROCs (at lesion levels), etc. (see bellow in the output descriptions).\n",
    "\n",
    "### 2.4 module \"statistical_tests\"\n",
    "The \"statistical_tests\" module is based on Bootstrap methods conceived by [Efron and Tibshirani](https://www.hms.harvard.edu/bss/neuro/bornlab/nb204/statistics/bootstrap.pdf). The bootstrap samples are computed with replacement.  It is recommended because Bootstrap methods are non-parametric (and does not make assumption on the distribution that should be verified, or use parameter that should be fitted), and can be applied generically using a common framework to wide range of observable (on AUC ROC and sensitivity of a given Operating Point of the ROC similarly). It takes a list of paths of pairs of npy saved vectors (in evaluate lesions or series) of n bootstrap samples metric values (AUC, accuracy, sensitivity, specificity....) and run all the list of tests. It implements each time 2 statistical tests using [label](https://docs.scipy.org/doc/scipy/reference/stats.html): \n",
    "* a superiority t-test with unequal variance (one-sided Welch t-test, 1 st prediction vs. 2nd)\n",
    "* a superiority test assuming equal variance (t-test, 1 st prediction vs. 2nd) \n",
    "It  return a csv with the result of all pairs of tests (p value, acceptance (strong, very strong, moderate, rejected)), but also a figure of the 2 bootstrapps distributions. \n",
    "This module can only be runned after \"evaluate_lesions\" and-or \"evaluate_series\" as it depends on their Bootstraps vector .npy file outputs (they are its only inputs). \n",
    "\n",
    "### 2.5 Outputs\n",
    "The input are stored in .\\data\\evaluate_series and .\\data\\evaluate_lesions directory, for series/patient level and nodules/lesions level input respectively.\n",
    "They are csv files, figures both saved in .png and .svg (allowing to further edit them in vectorial format), numpy arrays of bootstrapps samples (for statistical test). \n",
    "The saved figures are:\n",
    "* ROCs, with Operating Points (by default the Youden Index Maximum OP is given, but a list of OP threshold and labels can be given) \n",
    "* Precision Recall curve \n",
    "* the distribution of predictions for each labels\n",
    "* FROCs  (only at lesion level): with operating points at the point closest to 0.5FP/scan and 1FP/scan \n",
    "\n",
    "The saved csv are:\n",
    "* sample sizes: with total sample size, imbalance, and sample size in each class.\n",
    "* \"roc_CI_bootstrap_hanley\" csv: with AUC ROC confidence intervals (lower and upper) obtained using bootstrap methods and using [Hanley & McNeil method](https://pubs.rsna.org/doi/10.1148/radiology.143.1.7063747?url_ver=Z39.88-2003&rfr_id=ori:rid:crossref.org&rfr_dat=cr_pub%20%200pubmed). \n",
    "* \"operating_point_performances\" csv: with, for all operating points, the sens, spec, accuracy, their mean, lower and upper CI over n bootstrapps samples.  \n",
    "* \"operating_point_FROC_scores_at_0.5_and_1_FP_per_scan\"  (only at lesion level): with, for operating points at the point closest to 0.5FP/scan and 1FP/scan , the sens and exact FP/scan, their mean, lower and upper CI over n bootstrapps samples.\n",
    "* \"test_results\" csv (in statistical test folders):  with the result of all pairs of tests (p value, acceptance (strong, very strong, moderate, rejected)), but also a figure of the 2 bootstrapps distributions.\n",
    "\n",
    "The saved numpy (.npy) files are the n_boostrapped vectors of AUC, sens, spec and accuracy.\n",
    "\n",
    "\n",
    "## 3. Reproduce all papers figures, performances and tests in a single command from data inputs\n",
    "\n",
    "The `run_paper_evaluation.py` script is designed to produce all the evaluation performance analyses presented in the paper (give arXiv link). This script integrates all the evaluation modules of package and generates the required outputs of each figures of the paper in the associated directories.\n",
    "At series-patient and at nodule level, it generates all the output listed above for all figures of the paper, and all the statistical tests of the paper.\n",
    "All its parameters are strored in `config_paper.py`, notably the path where to get the input csv data for patients and nodules predictions and the path where to store the output, along with the list of figures, stats and tests to produce. This config is specific to the paper, just rewrite another config keeping the same structure for another set of evaluations.\n",
    "\n",
    "To make the whole paper evaluation, just run the following lines: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  run_paper_evaluation import paper_evaluation\n",
    "fast_computation = True  # Set to True for faster computation during testing/debugging (nb bootstrap samples reduced to 50 and fast FROC computation)\n",
    "paper_evaluation(fast_computation = fast_computation) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this demo example we settled the parameter \"fast_computation\" to True:  it allows a quite fast runing of the evaluation process (notably the number of bootstrapps samples is 50).\n",
    "\n",
    "Although qualitatively the results does not differ, if you want to reproduce the paper evaluation, you have to set the parameter \"fast_computation\" to False (and expect about a day of computation):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fast_computation = False  # Set to True for faster computation during testing/debugging (nb bootstrap samples reduced to 50 and fast FROC computation)\n",
    "paper_evaluation(fast_computation = fast_computation) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To visualize some figures output, just load the figure you want and display it (but the fastest way is to explore the output directories):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import config_paper as config\n",
    "import matplotlib.pyplot as plt\n",
    "image_path  = config.path_model_eval  / \"evaluate_series\" / \"figure_1\"/ \"model_prediction_test1\" / \"roc_curve_with_op_test1.png\"\n",
    "if image_path.exists():\n",
    "    img = plt.imread(image_path)\n",
    "    plt.imshow(img)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "else:\n",
    "    print(f\"figure not found at {image_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Submodules \"evaluate_series\", \"evaluate_lesions\", and \"statistical_tests\" independent usage\n",
    "\n",
    "We can now investigate the uses of the different submodules independently. To do so, you can either use the input csv provided in \\data\\data series or create a new one from scratch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import config_paper as config\n",
    "import pandas as pd\n",
    "from CADe_CADx_evaluation.evaluate_series import evaluate_series as eval_series\n",
    "import numpy as np\n",
    "\n",
    "path_data_series = config.path_model_eval / \"evaluate_series\" / \"series_to_use.csv\"\n",
    "sample_size = 1000\n",
    "np.random.seed(42) \n",
    "data = {\n",
    "    \"series_uid\": np.arange(1, sample_size + 1),  # Unique identifiers from 1 to 1000\n",
    "    \"prediction_to_use\": np.random.rand(sample_size),  # Random float values between 0 and 1\n",
    "    \"label_to_use\": np.random.randint(0, 2, size=sample_size),  # Random binary values (0 or 1)\n",
    "    \"subset_to_use\": np.random.choice([\"TRUE\", \"FALSE\"], size=sample_size, p=[0.9, 0.1])  # 90% TRUE, 10% FALSE\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "# where df[\"label_to_use\"] is 0, make df[\"prediction_to_use\"] random between 0 and 0.75\n",
    "df.loc[df[\"label_to_use\"] == 0, \"prediction_to_use\"] = np.random.rand(df.shape[0] - df[\"label_to_use\"].sum()) * 0.75\n",
    "# where df[\"label_to_use\"] is 1, make df[\"prediction_to_use\"] random between 0.25 and 1\n",
    "df.loc[df[\"label_to_use\"] == 1, \"prediction_to_use\"] = 0.25 + np.random.rand(df[\"label_to_use\"].sum()) * 0.75\n",
    "df.to_csv(path_data_series, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Submodule \"evaluate_series\"\n",
    "\n",
    "We can run the evaluation of series prediction we just created, by defining the arguments values and running:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from CADe_CADx_evaluation.evaluate_series import evaluate_series as eval_series\n",
    "path_data_series = config.path_model_eval / \"evaluate_series\" / \"series_to_use.csv\"\n",
    "path_model_evaluate_series = config.path_model_eval / \"evaluate_series\" / \"figure_test\"\n",
    "set_name = \"subset_to_use\"\n",
    "prediction = \"prediction_to_use\"\n",
    "label_name = \"label_to_use\"\n",
    "operating_point_thresholds = [0,]\n",
    "operating_point_labels = [\"Youden Index Max\"]\n",
    "nb_bootstrap_samples = 500\n",
    "confidence_threshold = 0.95\n",
    "eval_series.evaluate_serie_main(path_data_series ,  # path_to_load_csv_serie,\n",
    "                                path_model_evaluate_series ,  # expdir,\n",
    "                                set_name,  # set_name,\n",
    "                                prediction,  # prediction,\n",
    "                                label_name,  # label_name,\n",
    "                                operating_point_thresholds,  # operating_point_thresholds,\n",
    "                                operating_point_labels,\n",
    "                                nb_bootstrap_samples,  # nb_bootstrap_samples,\n",
    "                                confidence_threshold,)  # confidence_threshold,config.confidence_threshold,  # confidence_threshold,\n",
    "image_path  = config.path_model_eval  / \"evaluate_series\" / \"figure_test\"/ \"prediction_to_use_subset_to_use\" / \"roc_curve_with_op_subset_to_use.png\"\n",
    "if image_path.exists():\n",
    "    img = plt.imread(image_path)\n",
    "    plt.imshow(img)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "else:\n",
    "    print(f\"figure not found at {image_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Submodule \"evaluate_lesions\"\n",
    "\n",
    "### 4.3 Submodule \"statistical_tests\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
